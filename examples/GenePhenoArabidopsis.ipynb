{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cPickle, os, sys\n",
    "import random\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "from ddlite import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Importing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Thomas data folder \n",
    "DATA_FOLDER='/Users/thomaspalomares/Desktop/Stanford/RA/multisentences/data/'\n",
    "#Tanya data folder\n",
    "# DATA_FOLDER='/Users/tanyaberardini/Desktop/DeepDive/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #To be modified to conserve correct doc_ids (in particular, write in different txt files)\n",
    "# nb_lines=0\n",
    "# nb_error_parsing=0\n",
    "# with open(DATA_FOLDER+'pmc/json/output_plant.json', 'rb') as json_file:\n",
    "#     with open(DATA_FOLDER+'pmc/text/output_plant.text', 'wb') as text_write:\n",
    "#         docs={}\n",
    "#         for line in json_file:\n",
    "#             nb_lines+=1\n",
    "#             doc_id_read=str(json.loads(line)['doc-id'].encode('utf-8').strip())\n",
    "# #             try:\n",
    "#             #Because this doc is too big\n",
    "#             if doc_id_read=='10.1093.jxb.eru198':\n",
    "#                 if random.random() <0.5:\n",
    "#                     doc_id_read += '_1'\n",
    "#                 else:\n",
    "#                     doc_id_read += '_2'\n",
    "#             if doc_id_read in docs:\n",
    "#                 docs[doc_id_read] += unicode(str(json.loads(line)['content'].encode('utf-8').strip()) + '.\\n', errors='ignore')\n",
    "#             else:\n",
    "#                 docs[doc_id_read] = unicode(str(json.loads(line)['content'].encode('utf-8').strip()) + '.\\n', errors='ignore')\n",
    "# #                 text_write.write(unicode(str(json.loads(line)['content'].encode('utf-8').strip()) + '.\\n', errors='ignore'))\n",
    "# #             except:\n",
    "# #                 nb_error_parsing +=1\n",
    "# #                 print \"error parsing\"\n",
    "# print nb_lines\n",
    "# print nb_error_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test = SentenceParser()\n",
    "# test.parse(docs)\n",
    "# # dp = DocParser(DATA_FOLDER + 'pmc/text/', TextParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl_f=DATA_FOLDER+'pmc/pickle/pkl_sentences'\n",
    "sents=[]\n",
    "count_loop=0\n",
    "try:\n",
    "    with open(pkl_f, 'rb') as f:\n",
    "        sents = cPickle.load(f)\n",
    "except:\n",
    "    print\"parsing data\"\n",
    "    for doc_id_loop in docs:\n",
    "        count_loop+=1\n",
    "        print count_loop\n",
    "        for j in test.parse(docs[doc_id_loop], doc_id=doc_id_loop):\n",
    "            sents.append(j)\n",
    "    with open(pkl_f, 'w+') as f:\n",
    "        cPickle.dump(sents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence(words=[u'It', u'Takes', u'Two', u'to', u'Tango', u':', u'A', u'New', u'Partner', u'in', u'Amylose', u'Synthesis', u'.'], lemmas=[u'it', u'take', u'two', u'to', u'Tango', u':', u'A', u'New', u'Partner', u'in', u'Amylose', u'synthesis', u'.'], poses=[u'PRP', u'VBZ', u'CD', u'TO', u'NNP', u':', u'NNP', u'NNP', u'NNP', u'IN', u'NNP', u'NN', u'.'], dep_parents=[2, 0, 2, 5, 2, 5, 9, 9, 5, 12, 12, 9, 2], dep_labels=[u'nsubj', u'ROOT', u'dobj', u'case', u'nmod', u'punct', u'compound', u'compound', u'dep', u'case', u'compound', u'nmod', u'punct'], sent_id=1, doc_id='PBIOLOGY-D-14-02490', text=u'It Takes Two to Tango: A New Partner in Amylose Synthesis.', token_idxs=[153, 156, 162, 166, 169, 174, 176, 178, 182, 190, 193, 201, 210])\n"
     ]
    }
   ],
   "source": [
    "print sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Think about how to add doc_ids. \n",
    "#In particular, look at the small code in parseDocSentences()\n",
    "# %time sents = dp.parseDocSentences()\n",
    "# print sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate mention Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#What is the exact schema of genes ?\n",
    "genes=[]\n",
    "for row_genes in [line.rstrip().split('\\t') for line in open(DATA_FOLDER + '/dicts/list_genes.txt')]:\n",
    "    for gene in row_genes:\n",
    "        genes.append(gene)\n",
    "        \n",
    "#Removing blacklist_words\n",
    "blacklist_words = [line.rstrip().split('\\t')[1].lower() for line in open(DATA_FOLDER + '/dicts/blacklist_words.txt')][1:]\n",
    "genes_filtered = [x.lower() for x in genes if x not in blacklist_words]\n",
    "\n",
    "gene_dm = DictionaryMatch(label='GeneName', dictionary=genes_filtered, ignore_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary match should provide fairly high recall, but we may still miss some candidates. We know that gene names are named nouns and are often all uppercase. Let's use DDLite's compositional matcher operations to handle this. First, we'll write a matcher to find all nouns using the parts-of-speech tags. Then, we'll use a filter to find uppercase sequences. Finally, we'll use a filter to make sure each match has at least 3 characters. We pass noun_rm to up_rm, and up_rm to the final filter to compose them with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noun_regex = RegexNgramMatch(label='Nouns', regex_pattern=r'[A-Z]?NN[A-Z]?', ignore_case=True, match_attrib='poses')\n",
    "up_regex = RegexFilterAll(noun_regex, label='Upper', regex_pattern=r'[A-Z]+([0-9]+)?([A-Z]+)?([0-9]+)?$', ignore_case=False, match_attrib='words')\n",
    "multi_regex = RegexFilterAll(up_regex, label='Multi', regex_pattern=r'[a-z0-9]{3,}', ignore_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want matches both from the dictionary and the uppercase-noun-phrase-matcher we just built, we'll use the union object to create a matcher for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GM = Union(gene_dm, multi_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pheno extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Schema is: HPO_ID | NAME | TYPE (exact, lemma)\n",
    "phenos_deepdive = [line.rstrip().split('\\t')[1] for line in open(DATA_FOLDER + '/dicts/pheno_terms.tsv')]\n",
    "phenos_arabidopsis = [line.rstrip() for line in open(DATA_FOLDER + '/dicts/list_phenotypes_arabidopsis_filtered.txt')]\n",
    "phenos_worm_variants = [line.rstrip() for line in open(DATA_FOLDER + '/dicts/worm_variants_unix.txt')]\n",
    "phenos_all_eq_dict = [pheno.strip() for line in open(DATA_FOLDER + 'dicts/phenotypes_all_eq_dict.txt') for pheno in line.rstrip().split(';')]\n",
    "\n",
    "phenos_tot = phenos_deepdive + phenos_arabidopsis + phenos_worm_variants + phenos_all_eq_dict\n",
    "\n",
    "#Removing the phenotypes that are in the blacklist.\n",
    "blacklist_words = [line.rstrip().split('\\t')[1].lower() for line in open(DATA_FOLDER + '/dicts/blacklist_words.txt')][1:]\n",
    "phenos_tot_filtered = [x.lower() for x in phenos_tot if x not in blacklist_words]\n",
    "\n",
    "PM = DictionaryMatch(label='PhenoName', dictionary=phenos_tot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence, we extract a candidate relation for each pair (gene, phenotype) appearing in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = Relations(sents, GM, PM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".node {\n",
       "  cursor: pointer;\n",
       "}\n",
       "\n",
       ".node circle {\n",
       "  fill: #fff;\n",
       "  stroke: steelblue;\n",
       "  stroke-width: 3px;\n",
       "}\n",
       "\n",
       ".node text {\n",
       "  font: 12px sans-serif;\n",
       "}\n",
       "\n",
       ".edge {\n",
       "  fill: none;\n",
       "  stroke: #ccc;\n",
       "  stroke-width: 2px;\n",
       "  cursor: pointer;\n",
       "}\n",
       "\n",
       ".highlight {\n",
       "  stroke: red;\n",
       "  stroke-width: 3px;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<!--Provide the canvas id (twice) and the words via python string formatting here--!>\n",
       "<div id=\"tree-chart-843864449012152995\"></div>\n",
       "<div id=\"raw-seq-843864449012152995\">\n",
       "<span class=\"word-843864449012152995-0\">Amylose</span> <span class=\"word-843864449012152995-1\">is</span> <span class=\"word-843864449012152995-2\">composed</span> <span class=\"word-843864449012152995-3\">of</span> <span class=\"word-843864449012152995-4\">long</span> <span class=\"word-843864449012152995-5\">linear</span> <span class=\"word-843864449012152995-6\">chains</span> <span class=\"word-843864449012152995-7\">with</span> <span class=\"word-843864449012152995-8\">few</span> <span class=\"word-843864449012152995-9\">branch</span> <span class=\"word-843864449012152995-10\">points</span> <span class=\"word-843864449012152995-11\">,</span> <span class=\"word-843864449012152995-12\">while</span> <span class=\"word-843864449012152995-13\">amylopectin</span> <span class=\"word-843864449012152995-14\">has</span> <span class=\"word-843864449012152995-15\">shorter</span> <span class=\"word-843864449012152995-16\">chains</span> <span class=\"word-843864449012152995-17\">and</span> <span class=\"word-843864449012152995-18\">many</span> <span class=\"word-843864449012152995-19\">more</span> <span class=\"word-843864449012152995-20\">branch</span> <span class=\"word-843864449012152995-21\">points</span> <span class=\"word-843864449012152995-22\">.</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "$.getScript(\"http://d3js.org/d3.v3.min.js\", function () {\n",
       "// See http://bl.ocks.org/d3noob/8375092\n",
       "// Three vars need to be provided via python string formatting:\n",
       "var chartId = \"843864449012152995\";\n",
       "var root = {\"attrib\": {\"token_idx\": \"4651\", \"word\": \"composed\", \"dep_label\": \"ROOT\", \"pos\": \"VBN\", \"lemma\": \"compose\", \"word_idx\": \"2\", \"dep_parent\": \"0\"}, \"children\": [{\"attrib\": {\"token_idx\": \"4640\", \"word\": \"Amylose\", \"dep_label\": \"nsubjpass\", \"pos\": \"NNP\", \"lemma\": \"Amylose\", \"word_idx\": \"0\", \"dep_parent\": \"3\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4648\", \"word\": \"is\", \"dep_label\": \"auxpass\", \"pos\": \"VBZ\", \"lemma\": \"be\", \"word_idx\": \"1\", \"dep_parent\": \"3\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4675\", \"word\": \"chains\", \"dep_label\": \"nmod\", \"pos\": \"NNS\", \"lemma\": \"chain\", \"word_idx\": \"6\", \"dep_parent\": \"3\"}, \"children\": [{\"attrib\": {\"token_idx\": \"4660\", \"word\": \"of\", \"dep_label\": \"case\", \"pos\": \"IN\", \"lemma\": \"of\", \"word_idx\": \"3\", \"dep_parent\": \"7\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4663\", \"word\": \"long\", \"dep_label\": \"amod\", \"pos\": \"JJ\", \"lemma\": \"long\", \"word_idx\": \"4\", \"dep_parent\": \"7\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4668\", \"word\": \"linear\", \"dep_label\": \"amod\", \"pos\": \"JJ\", \"lemma\": \"linear\", \"word_idx\": \"5\", \"dep_parent\": \"7\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4698\", \"word\": \"points\", \"dep_label\": \"nmod\", \"pos\": \"NNS\", \"lemma\": \"point\", \"word_idx\": \"10\", \"dep_parent\": \"7\"}, \"children\": [{\"attrib\": {\"token_idx\": \"4682\", \"word\": \"with\", \"dep_label\": \"case\", \"pos\": \"IN\", \"lemma\": \"with\", \"word_idx\": \"7\", \"dep_parent\": \"11\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4687\", \"word\": \"few\", \"dep_label\": \"amod\", \"pos\": \"JJ\", \"lemma\": \"few\", \"word_idx\": \"8\", \"dep_parent\": \"11\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4691\", \"word\": \"branch\", \"dep_label\": \"compound\", \"pos\": \"NN\", \"lemma\": \"branch\", \"word_idx\": \"9\", \"dep_parent\": \"11\"}, \"children\": []}]}]}, {\"attrib\": {\"token_idx\": \"4704\", \"word\": \",\", \"dep_label\": \"punct\", \"pos\": \",\", \"lemma\": \",\", \"word_idx\": \"11\", \"dep_parent\": \"3\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4724\", \"word\": \"has\", \"dep_label\": \"advcl\", \"pos\": \"VBZ\", \"lemma\": \"have\", \"word_idx\": \"14\", \"dep_parent\": \"3\"}, \"children\": [{\"attrib\": {\"token_idx\": \"4706\", \"word\": \"while\", \"dep_label\": \"mark\", \"pos\": \"IN\", \"lemma\": \"while\", \"word_idx\": \"12\", \"dep_parent\": \"15\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4712\", \"word\": \"amylopectin\", \"dep_label\": \"nsubj\", \"pos\": \"NN\", \"lemma\": \"amylopectin\", \"word_idx\": \"13\", \"dep_parent\": \"15\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4736\", \"word\": \"chains\", \"dep_label\": \"dobj\", \"pos\": \"NNS\", \"lemma\": \"chain\", \"word_idx\": \"16\", \"dep_parent\": \"15\"}, \"children\": [{\"attrib\": {\"token_idx\": \"4728\", \"word\": \"shorter\", \"dep_label\": \"amod\", \"pos\": \"JJR\", \"lemma\": \"shorter\", \"word_idx\": \"15\", \"dep_parent\": \"17\"}, \"children\": []}]}, {\"attrib\": {\"token_idx\": \"4743\", \"word\": \"and\", \"dep_label\": \"cc\", \"pos\": \"CC\", \"lemma\": \"and\", \"word_idx\": \"17\", \"dep_parent\": \"15\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4764\", \"word\": \"points\", \"dep_label\": \"conj\", \"pos\": \"NNS\", \"lemma\": \"point\", \"word_idx\": \"21\", \"dep_parent\": \"15\"}, \"children\": [{\"attrib\": {\"token_idx\": \"4747\", \"word\": \"many\", \"dep_label\": \"amod\", \"pos\": \"JJ\", \"lemma\": \"many\", \"word_idx\": \"18\", \"dep_parent\": \"22\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4752\", \"word\": \"more\", \"dep_label\": \"amod\", \"pos\": \"JJR\", \"lemma\": \"more\", \"word_idx\": \"19\", \"dep_parent\": \"22\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"4757\", \"word\": \"branch\", \"dep_label\": \"compound\", \"pos\": \"NN\", \"lemma\": \"branch\", \"word_idx\": \"20\", \"dep_parent\": \"22\"}, \"children\": []}]}]}, {\"attrib\": {\"token_idx\": \"4770\", \"word\": \".\", \"dep_label\": \"punct\", \"pos\": \".\", \"lemma\": \".\", \"word_idx\": \"22\", \"dep_parent\": \"3\"}, \"children\": []}]};\n",
       "var highlightIdxs = [[14], [4]];\n",
       "\n",
       "// Highlight words / nodes\n",
       "var COLORS = [\"#ff5c33\", \"#ffcc00\", \"#33cc33\", \"#3399ff\"];\n",
       "function highlightWords() {\n",
       "  for (var i=0; i < highlightIdxs.length; i++) {\n",
       "    var c = COLORS[i];\n",
       "    var idxs = highlightIdxs[i];\n",
       "    for (var j=0; j < idxs.length; j++) {\n",
       "      d3.selectAll(\".word-\"+chartId+\"-\"+idxs[j]).style(\"stroke\", c).style(\"background\", c);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "// Constants\n",
       "var margin = {top: 20, right: 20, bottom: 20, left: 20},\n",
       "width = 800 - margin.left - margin.right,\n",
       "height = 350 - margin.top - margin.bottom,\n",
       "R = 5;\n",
       "\n",
       "// Create the d3 tree object\n",
       "var tree = d3.layout.tree()\n",
       "  .size([width, height]);\n",
       "\n",
       "// Create the svg canvas\n",
       "var svg = d3.select(\"#tree-chart-\" + chartId)\n",
       "  .append(\"svg\")\n",
       "  .attr(\"width\", width + margin.left + margin.right)\n",
       "  .attr(\"height\", height + margin.top + margin.bottom)\n",
       "  .append(\"g\")\n",
       "  .attr(\"transform\", \"translate(\" + margin.left + \",\" + margin.top + \")\");\n",
       "\n",
       "function renderTree() {\n",
       "  var nodes = tree.nodes(root),\n",
       "  edges = tree.links(nodes);\n",
       "\n",
       "  // Place the nodes\n",
       "  var nodeGroups = svg.selectAll(\"g.node\")\n",
       "    .data(nodes)\n",
       "    .enter().append(\"g\")\n",
       "    .attr(\"class\", \"node\")\n",
       "    .attr(\"transform\", function(d) { return \"translate(\" + d.x + \",\" + d.y + \")\"; });\n",
       "       \n",
       "  // Append circles\n",
       "  nodeGroups.append(\"circle\")\n",
       "    //.on(\"click\", function() {\n",
       "    //  d3.select(this).classed(\"highlight\", !d3.select(this).classed(\"highlight\")); })\n",
       "    .attr(\"r\", R)\n",
       "    .attr(\"class\", function(d) { return \"word-\"+chartId+\"-\"+d.attrib.word_idx; });\n",
       "     \n",
       "  // Append the actual word\n",
       "  nodeGroups.append(\"text\")\n",
       "    .text(function(d) { return d.attrib.word; })\n",
       "    .attr(\"text-anchor\", function(d) { \n",
       "      return d.children && d.children.length > 0 ? \"start\" : \"middle\"; })\n",
       "    .attr(\"dx\", function(d) { \n",
       "      return d.children && d.children.length > 0 ? R + 3 : 0; })\n",
       "    .attr(\"dy\", function(d) { \n",
       "      return d.children && d.children.length > 0 ? 0 : 3*R + 3; });\n",
       "\n",
       "  // Place the edges\n",
       "  var edgePaths = svg.selectAll(\"path\")\n",
       "    .data(edges)\n",
       "    .enter().append(\"path\")\n",
       "    .attr(\"class\", \"edge\")\n",
       "    .on(\"click\", function() {\n",
       "      d3.select(this).classed(\"highlight\", !d3.select(this).classed(\"highlight\")); })\n",
       "    .attr(\"d\", d3.svg.diagonal());\n",
       "}\n",
       "\n",
       "renderTree();\n",
       "highlightWords();\n",
       "});\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "R[0].render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# R.dump_candidates('pickle/relations.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddlite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a ddlite model from the previous candidates, on which we will extract features, define labeling functions and learn a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 143258 features for each of 7753 mentions\n"
     ]
    }
   ],
   "source": [
    "DDL = DDLiteModel(R)\n",
    "print \"Extracted {} features for each of {} mentions\".format(DDL.num_feats(), DDL.num_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a gold ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label here a certain amount of sentences that will allow us to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gold = np.zeros((DDL.num_candidates()))\n",
    "# gold[np.array([68,69,70,71,72,73,74,75,76,78,79,80,81,82,83,84,85,86,88,89,90,91])] = np.array([\n",
    "#      1,1,1,-1,1,-1,-1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,1,1,1])\n",
    "# DDL.set_gold_labels(gold)\n",
    "# DDL.set_holdout(p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sure MindTagger is installed. Hang on!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1200\"\n",
       "            src=\"http://DN51s5h3.SUNet:8971/#/mindtagger/cff402d13733f8c3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x12ae07350>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Yellow: gene. Blue: phenotype\n",
    "DDL.open_mindtagger(num_sample=200, width='100%', height=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing labeling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use data programming to learn a logistic regression model which will predict the probability of a candidate entity being a true gene mention. Since our training data is not manually labeled, we'll generate many (potentially noisy) labels as a surrogate for precise, manual labels. Feature extraction and model learning are very simple in ddlite. Writing labeling functions is where the real artistry comes in. One of ddlite's goals is to enable rapid prototyping, debugging, and experimenting with labeling functions. These can be used either to create a simple standalone app, or to plug into DeepDive. Labeling functions, or LFs, are functions that take an Candidate object. They must return 1 (for a positive label), 0 (for abstaining), or -1 (for a negative example). For now, we'll write a few simple LFs to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_window(m, key, n=3):\n",
    "    s = list(m.idxs)\n",
    "    b = len(m.lemmas) - np.max(s)\n",
    "    s.extend([np.max(s) + i for i in range(1, min(b,n+1))])\n",
    "    return key in [m.lemmas[i] for i in s]\n",
    "def pre_window(m, key, n=3):\n",
    "    s = list(m.idxs)\n",
    "    b = np.min(s)\n",
    "    s.extend([b - i for i in range(1, min(b,n+1))])\n",
    "    return key in [m.lemmas[i] for i in s]\n",
    "def stopper(m, stop):\n",
    "    return stop in [m.lemmas[i] for i in m.idxs]\n",
    "\n",
    "\n",
    "def LF_mutation(r):\n",
    "    return 1 if 'mutation' in [m.lemmas[m.dep_parents[i] - 1] for i in m.idxs] else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'relation_internal' object has no attribute 'idxs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-92c9964f4813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mLFs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLF_mutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# LFs = [LF_gene, LF_mutant, LF_express, LF_mutation, LF_dna, LF_rna, LF_snp]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mDDL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_lfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLFs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/thomaspalomares/Desktop/Stanford/RA/multisentences/ddlite/ddlite.py\u001b[0m in \u001b[0;36mapply_lfs\u001b[0;34m(self, lfs_f, clear)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlfs_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0madd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m     \u001b[0madd_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlfs_f\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_lf_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-aec00072ac5e>\u001b[0m in \u001b[0;36mLF_mutation\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Return one if the word 'mutation' is in the dependency path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLF_mutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'mutation'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep_parents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomaspalomares/Desktop/Stanford/RA/multisentences/ddlite/ddlite.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'relation_internal' object has no attribute 'idxs'"
     ]
    }
   ],
   "source": [
    "LFs= [LF_mutation]\n",
    "# LFs = [LF_gene, LF_mutant, LF_express, LF_mutation, LF_dna, LF_rna, LF_snp]\n",
    "DDL.apply_lfs(LFs, clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12,4)\n",
    "%time DDL.learn_weights(sample=False, maxIter=500, alpha=0.5, verbose=True, plot=True, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt with multi sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First basic version when some end of sentences \".\" are replaced by \";\" to easily have candidates covered in multisentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #To be modified to conserve correct doc_ids (in particular, write in different txt files)\n",
    "# nb_lines=0\n",
    "# nb_error_parsing=0\n",
    "# with open(DATA_FOLDER+'pmc/json/output_plant.json', 'rb') as json_file:\n",
    "#     with open(DATA_FOLDER+'pmc/text/output_plant.text', 'wb') as text_write:\n",
    "#         docs_multisentences={}\n",
    "#         for line in json_file:\n",
    "#             nb_lines+=1\n",
    "#             doc_id_read=str(json.loads(line)['doc-id'].encode('utf-8').strip())\n",
    "# #             try:\n",
    "#             #Because this doc is too big\n",
    "#             if doc_id_read=='10.1093.jxb.eru198':\n",
    "#                 if random.random() <0.5:\n",
    "#                     doc_id_read += '_1'\n",
    "#                 else:\n",
    "#                     doc_id_read += '_2'\n",
    "#             if doc_id_read in docs_multisentences:\n",
    "#                 if random.random() < 0.5:\n",
    "#                     docs_multisentences[doc_id_read] += unicode(str(json.loads(line)['content'].encode('utf-8').strip()) + '.\\n', errors='ignore')\n",
    "#                 else:\n",
    "#                     docs_multisentences[doc_id_read] += unicode(str(json.loads(line)['content'].encode('utf-8').strip()) + ';\\n', errors='ignore')\n",
    "#             else:            \n",
    "#                 if random.random() < 0.5:\n",
    "#                     docs_multisentences[doc_id_read] = unicode(str(json.loads(line)['content'].encode('utf-8').strip()) + '.\\n', errors='ignore')\n",
    "#                 else:\n",
    "#                     docs_multisentences[doc_id_read] = unicode(str(json.loads(line)['content'].encode('utf-8').strip()) + ';\\n', errors='ignore') \n",
    "\n",
    "                    \n",
    "# print nb_lines\n",
    "# print nb_error_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pkl_f=DATA_FOLDER+'pmc/pickle/pkl_sentences_multi'\n",
    "# sents_multi=[]\n",
    "# count_loop=0\n",
    "# try:\n",
    "#     with open(pkl_f, 'rb') as f:\n",
    "#         sents_multi = cPickle.load(f)\n",
    "# except:\n",
    "#     print\"parsing data\"\n",
    "#     for doc_id_loop in docs:\n",
    "#         count_loop+=1\n",
    "#         print count_loop\n",
    "#         for j in test.parse(docs_multisentences[doc_id_loop], doc_id=doc_id_loop):\n",
    "#             sents_multi.append(j)\n",
    "#     with open(pkl_f, 'w+') as f:\n",
    "#         cPickle.dump(sents_multi, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print sents_multi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# R_multi = Relations(sents_multi, GM, PM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DDL_multi = DDLiteModel(R_multi)\n",
    "# print \"Extracted {} features for each of {} mentions\".format(DDL_multi.num_feats(), DDL_multi.num_candidates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DDL_multi.open_mindtagger(num_sample=200, width='100%', height=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
